\chapter{Algèbre linéaire}
Ce chapitre traite de l’algèbre linéaire exacte et symbolique, c’est-à-dire sur
des anneaux propres au calcul formel, tels que $Z$, des corps finis, des anneaux de
polynômes. Nous présentons les constructions sur les matrices et leurs espaces ainsi que les
opérations de base, puis les différents calculs possibles sur ces matrices, regroupés en deux thèmes : ceux liés à l’élimination de Gauss et aux transformations par équivalence à gauche, et ceux liés aux valeurs et espaces
propres et aux transformations de similitude.
\section{Constructions et manipulations élémentaires}
\subsection{ Espaces de vecteurs, de matrices}
De la même façon que pour les polynômes, les vecteurs et les matrices sont
manipulés comme des objets algébriques appartenant à un espace. Si les coefficients
appartiennent à un corps $K$, c’est un espace vectoriel sur $K$ ; s’ils appartiennent
à un anneau, c’est un $K-module$ libre.
\textbf{Groupes de matrices.} On poura par ailleur définir des sous-groupes de l'espace total des matrices. Ainsi le constructeur 
\subsection{ Construction des matrices et des vecteurs}
Les matrices et les vecteurs peuvent naturellement être générés comme des éléments d’un espace en fournissant la liste des coefficients en arguments. Pour les matrices, ceux-ci seront lus par ligne :
\subsection{ Manipulations de base et arithmétique sur les matrices}
\textbf{Indices et accès aux coefficients.} L'accès aux coefficients ainsi qu’à des
sous-matrices extraites se fait de façon unifiée par l’opérateur crochet $A$ $\left[i, j\right]$,selon les 
conventions usuelles de Python. Les indices de ligne $i$ et de colonne $j$ peuvent être des entiers (pour 
l’accès à des coefficients) ou des intervalles sous la forme $1:3$ (on rappelle que par convention, en Python 
les indices commencent à $0$, et les intervalles sont toujours inclusifs pour la borne inférieure et exclusifs 
pour la borne supérieure). L’intervalle « : » sans bornes correspond à la totalité des indices possibles dans la 
dimension considérée. La notation $a:b:k$ permet d’accéder aux indices compris entre $a$ et $b-1$ par pas de 
$k$. Enfin, les indices négatifs sont aussi valides, et permettent de parcourir les indices à partir de la
fin. Ainsi $A$ $\left[-2,:\right]$ correspond à l’avant dernière ligne. L'accès à ces sous-matrices
se fait aussi bien en lecture qu’en écriture. On peut par exemple modifier une colonne donnée de la façon 
suivante :
\subsection{ Opérations de base sur les matrices}
Les opérations arithmétiques sur les matrices se font avec les opérateurs usuels +,-,$\ast$,\^. L’inverse 
d’une matrice $A$ peut s’écrire aussi bien $A^{-1}$ que $~A$. Lorsque $a$ est un scalaire et $A$ une matrice, 
l’opération $a*A$ correspond à la multiplication externe de l’espace de matrices. Pour les autres opérations où 
un scalaire a est fourni en lieu et place d’une matrice (par exemple l’opération a+A), il est considéré comme la 
matrice scalaire correspondante $aI_{n}$ si a $a\neq 0$ et les dimensions le permettent. Le produit élément par 
élément de deux matrices s’effectue avec l’opération elementwise\_product.
\section{ Calculs sur les matrices}
En algèbre linéaire, les matrices peuvent être utilisées pour représenter aussi bien des familles de vecteurs, 
des systèmes d’équations linéaires, des applications linéaires ou des sous-espaces. Ainsi, le calcul d’une 
propriété comme le rang d’une famille, la solution d’un système, les espaces propres d’une application linéaire, 
ou la dimension d’un sous-espace se ramènent à des transformations sur ces matrices révélant cette propriété. 
Ces transformations correspondent à des changements de base, vus au niveau.
\\
Ces transformations correspondent à des changements de base, vus au niveau
matriciel comme des transformations d’équivalence : $B = PAQ^{-1}$ , où $P$ et $Q$ sont
des matrices inversibles. Deux matrices sont dites équivalentes s’il existe une telle

transformation pour passer de l’une à l’autre. On peut ainsi former des classes
d’équivalence pour cette relation, et l’on définit des formes normales, permettant
de caractériser de manière unique chaque classe d’équivalence. Dans ce qui suit,
nous présentons l’essentiel des calculs sur les matrices disponibles avec SymPy, sous
l’angle de deux cas particuliers de ces transformations :

\begin{itemize}
 	 \item Les transformations d’équivalence à gauche, de la forme $B = UA$, qui
révèlent les propriétés caractéristiques pour les familles de vecteurs, telles
que le rang (nombre de vecteurs linéairement indépendants), le déterminant
(volume du parallélépipède décrit par la famille de vecteurs), le profil de rang
(premier sous-ensemble de vecteurs formant une base), . . . L’élimination de
Gauss est l’outil central pour ces transformations, et la forme échelonnée
réduite (forme de Gauss-Jordan dans un corps ou forme de Hermite dans $\mathbb{Z}$)
est la forme normale. En outre, ces transformations servent à la résolution
des systèmes linéaires.
	 \item  Les transformations de similitude, de la forme $B = UAU^{-1}$ , qui révèlent les
propriétés caractéristiques des matrices représentant des endomorphismes,
comme les valeurs propres, les espaces propres, les polynômes minimal et
caractéristique, . . . La forme de Jordan ou la forme de Frobenius, selon les
domaines de calcul, seront les formes normales pour ces transformations.
\end{itemize}

La forme de Gram-Schmidt est une autre décomposition basée sur les transfomations d’équivalence à gauche, transformant une matrice en un ensemble de vecteurs orthogonaux.

\subsection{ Élimination de Gauss, forme échelonnée}

\begin{flushleft}
\textbf{Élimination de Gauss et équivalence à gauche.} L’élimination de Gauss est l’une des opérations fondamentales en algèbre linéaire car elle permet d’accéder à une représentation de la matrice à la fois plus adaptée au calcul, comme la résolution de systèmes, et révélant certaines de ses propriétés fondamentales,
comme le rang, le déterminant, le profil de rang, etc. Les opérations de base pour l’élimination sont les opérations élémentaires sur les lignes :
\end{flushleft}

\begin{flushleft}
\textbf{Élimination de Gauss-Jordan.} La transformation de Gauss-Jordan est similaire à celle de Gauss, en ajoutant à $G_{x,k}$ les transvections correspondant aux lignes d’indice $i < k$ ; cela revient à éliminer les coefficients d’une colonne au-dessus et au-dessous du pivot. Si de plus on divise chaque ligne par son pivot, on obtient alors une forme échelonnée dite réduite encore appelée forme de Gauss-Jordan. Pour toute classe d’équivalence de matrices, il existe une unique matrice sous cette forme ; il s’agit donc d’une forme normale.
\end{flushleft}

\begin{flushleft}
\textbf{Forme échelonnée dans les anneaux euclidiens.} Dans un anneau euclidien, les coefficients non nuls ne sont pas nécessairement inversibles, et l’élimination de Gauss consisterait donc à choisir le premier élément inversible de la colonne courante pour pivot. Ainsi certaines colonnes non nulles peuvent ne pas contenir
de pivot et l’élimination n’est alors plus possible. Il est cependant toujours possible de définir une transformation unimodulaire éliminant le coefficient de tête d’une ligne avec celui d’une autre, grâce à
l’algorithme d’Euclide étendu.
\end{flushleft}

\begin{flushleft}
\textbf{Facteurs invariants et forme normale de Smith.}
\end{flushleft}

\subsection{ Résolution de systèmes ; image et base du noyau}
\subsection{Valeurs propres, forme de Jordan et transformations de similitude}
Lorsque l’on interprète une matrice carrée comme un opérateur linéaire (un endomorphisme), elle n’en est que la représentation dans une base donnée. Tout changement de base correspond à une transformation de similitude 
$B = U^{-1}AU$ de la matrice. Les deux matrices $A$ et $B$ sont alors dites semblables. Ainsi les propriétés de l’opérateur linéaire, qui sont indépendantes de la base, sont révélées par l’étude des invariants de similitude de la matrice.
par l’étude des invariants de similitude de la matrice. Parmi ces invariants, les plus simples sont le rang et le déterminant. En effet les matrices $U$ et $U^{-1}$ étant inversibles, le rang de $U^{-1}AU$ égale le rang de $A$. De plus $det\left( U^{-1}AU\right)  = det\left( U^{-1}\right) det \left(A det(U) = det(U^{-1}U\right) det\left( A\right) = det\left(A\right)$. De
la même façon, le polynôme caractéristique de la matrice $A$, défini par 
 est aussi invariant par transformation de similitude :
%
%\[
% det\left(xId − U^{-1}AU\right) = det\left(U^{-1} \left( xId − A\right) U \left) = det\left( xId − A\right) .
%\]

Par conséquent, les valeurs caractéristiques d’une matrice, définies comme les racines du polynôme caractéristique dans son corps de décomposition, sont donc aussi des invariants de similitude. Par definition, un scalaire $\lambda$ est une valeur propre d’une matrice $A$ s’il existe un vecteur non nul $u$ tel que $Au = \lambda u$. L'espace propre associé à une valeur propre $\lambda$ est l’ensemble des vecteurs $u$ tels que $Au = \lambda u$. C'est un sous-espace vectoriel defini par $E_{\lambda} = Ker(\lambda Id - A)$. \\
Les valeurs propres coïncident avec les valeurs caractéristiques :
\[
 det(\lambda Id - A) = 0 \Leftrightarrow dim(Ker(\lambda Id - A)) > 1 \Leftrightarrow \exists u 6= 0, \lambda u - Au = 0.
\]
Ces deux points de vue correspondent respectivement à l’approche algébrique et géométrique des valeurs propres. Dans le point de vue géométrique, on s’intéresse à l’action de l’opérateur linéaire $A$ sur les vecteurs de l’espace avec plus de précision que dans le point de vue algébrique. En particulier on distingue les notions de
multiplicité algébrique, correspondant à l’ordre de la racine dans le polynôme caractéristique, de la multiplicité géométrique, correspondant à la dimension du sous-espace propre associé à la valeur propre. Pour les matrices diagonalisables, ces deux notions sont équivalentes. Dans le cas contraire, la multiplicité géométrique est toujours inférieure à la multiplicité algébrique. Le point de vue géométrique permet de décrire plus en détail la structure de la matrice. Par ailleurs, il donne des algorithmes beaucoup plus rapides pour le
calcul des valeurs et espaces propres, et des polynômes caractéristique et minimal.
\\
\textbf{Espaces invariants cycliques, et forme normale de Frobenius.} Soit $A$ Soit une matrice $n x n$ sur un 
corps $K$ et $u$ un vecteur de $K^{n}$. La famille de vecteurs $u$, $Au$, $A^{2}u$, ..., $A^{n}u$, appelée suite de 
Krylov, est liée (comme famille de $n + 1$ vecteurs en dimension $n$). Soit $d$ tel que $A^{d} u$ soit le
premier vecteur de la séquence linéairement dépendant avec ses prédécesseurs $u$, $Au$, ..., $A^{d-1}u$.On 
écrira
\[
	A^{d}u = \sum_{i=0}^{d-1} \alpha_{i} A^{i}u
\]
cette relation de dépendance linéaire. Le polynôme $\varphi_{A, u}\left(x\right) = x^{d} = \sum_{i=0}^{d-1} \alpha_{i}x^{i}$ , qui vérifie $\varphi_{A, u}\left(A\right)u = 0$ est donc un polynôme unitaire annulateur de la suite de Krylov et de degré minimal. On l’appelle le polynôme minimal du vecteur $u$ (sous
entendu, relativement à la matrice $A$). L’ensemble des polynômes annulateurs de u forme un idéal de $K\left[X\right]$, engendré par $\varphi_{A, u}$ .
Le polynôme minimal de la matrice $A$ est défini comme le polynôme unitaire
$\varphi_{A}\left(x\right)$ de plus petit degré annulant la matrice $A$ : $\varphi_{A}\left(A\right) = 0$. En particulier, en appliquant $\varphi_{A}\left(A\right)$ au vecteur $u$, on constate que $\varphi_{A}\left(A\right)$ est un polynôme annulateur de la suite de Krylov. Il est donc nécessairement un multiple du polynôme minimal de u. On peut en outre montrer (cf. exercice ) qu’il existe un vecteur $\overline{u}$ tel que
\begin{equation}
 \varphi_{A, \overline{u}} = \varphi_{A}.
\end{equation}
Lorsque le vecteur $u$ est choisi aléatoirement, la probabilité qu’il satisfasse l’équation (1.1) est d’autant plus grande que la taille du corps est grande (on peut montrer qu’elle est au moins $1-\frac{n}{\vert K \vert}$ 
\begin{exercise}
Montrons qu’il existe toujours un vecteur u dont le polynôme minimal coïncide avec le polynôme minimal de la matrice.
 \begin{enumerate}
    \item coïncide avec le polynôme minimal de la matrice. 1. Soit $\left(e_{1},..., e_{n}\right)$ une base de l’espace vectoriel. Montrer que $\varphi_{A}$ coïncide avec le ppcm des $\varphi_{A, e_{i}}$ .
    \item  Dans le cas particulier où $\varphi_{A}$ est une puissance d’un polynôme irréductible, montrer qu’il existe un indice $i_{0}$ tel que $\varphi_{A} =\varphi_{A, e_{i0}}$.
    \item 
    \item 
    \item 
  \end{enumerate}
\end{exercise}
\textbf{Facteurs invariants et invariants de similitude.}  Une propriété importante relie les invariants de similitude et les facteurs invariants vus dans la section  Les invariants de similitude d’une matrice  à coefficients dans
\begin{theorem}
Les invariants de similitude d’une matrice $A$ à coefficients dans un corps correspondent aux facteurs invariants de sa matrice caractéristique $xId - A$.
\end{theorem}
\textbf{Valeurs propres, vecteurs propres}. Si l’on décompose le polynôme minimal en facteurs irréductibles, 
$\varphi_{1}$ = $\psi_{1}^{m_{1}}$, $\psi_{2}^{m_{2}}$, $\psi_{3}^{m_{3}}$,..., $\psi_{1}^{m_{s}}$ alors tous les facteurs invariants s’écrivent sous la forme.
\\
\textbf{Forme de Jordan.} Lorsque le polynôme minimal est scindé mais ayant des
facteurs avec des multiplicités supérieures à 1, la forme intermédiaire (8.3) n’est
pas diagonale. On montre alors qu’il n’existe pas de transformation de similitude
la rendant diagonale, la matrice initiale n’est donc pas diagonalisable. On peut
en revanche la trigonaliser, c’est-à-dire la rendre triangulaire supérieure, telle que
les valeurs propres apparaissent sur la diagonale. Parmi les différentes matrices
triangulaires possibles, la plus réduite de toutes est la forme normale de Jordan.
\\
\textbf{Forme normale primaire.} Pour être complet, il faut mentionner une dernière
forme normale qui généralise la forme de Jordan dans le cas quelconque où le
polynôme minimal n’est pas scindé. Pour un polynôme irréductible $P$ de degré $k$,
on définit le bloc de Jordan de multiplicité $m$ comme la matrice $J_{P,m}$ de dimension
$km x km$ vérifiant.
\\
unique à une permutation des blocs diagonaux près. L’unicité de ces formes normales permet en particulier de tester si deux matrices sont semblables, et par la même occasion de produire une matrice de
passage entre l’une et l’autre.

\begin{exercise}
Écrire un programme qui détermine si deux matrices $A$ et $B$ sont semblables et renvoie la matrice $U$ de passage telle que $A = U^{-1}BU$ (on pourra renvoyer None dans le cas où les matrices ne sont pas semblables).
\end{exercise}